# Neural Network From Scratch 

This repository contains a **fully implemented Neural Network from scratch**, 
built without using high-level deep learning libraries such as TensorFlow or PyTorch for the core learning logic.

The goal of this project is to **understand and demonstrate the internal working of neural networks**, 
including forward propagation, loss computation, backpropagation, and weight updates.

---

##  Project Motivation

Most machine learning projects rely heavily on frameworks, which often hide the internal mechanics.  
This project focuses on **learning by building** — implementing every critical component manually to gain a deep conceptual understanding.

This is especially useful for:
- Machine Learning interviews
- Core ML/DL understanding
- Academic clarity
- Building strong fundamentals before using frameworks

---

##  What This Project Implements

- Neural Network architecture from scratch
- Forward propagation
- Activation functions
- Loss function calculation
- Backpropagation algorithm
- Gradient descent–based weight updates
- Training loop
- Prediction logic

> No high-level deep learning APIs are used for the learning process.

---

## Tech Stack

- Python
- NumPy
- colab Notebook

---

